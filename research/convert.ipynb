{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchlibrosa\n",
    "import numpy as np\n",
    "import soundfile\n",
    "import os\n",
    "import coremltools as ct\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveToSpectrogram(nn.Module):\n",
    "\n",
    "    def __init__( self, n_fft, hop_length, center=False ):\n",
    "        super(WaveToSpectrogram, self).__init__()\n",
    "\n",
    "        self.spec_extractor = torchlibrosa.stft.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return self.spec_extractor( x )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/audio/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/Users/hugo/opt/anaconda3/envs/audio/lib/python3.8/site-packages/torch/serialization.py:359: UserWarning: Couldn't retrieve source code for container of type WaveToSpectrogram. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "model = WaveToSpectrogram( n_fft=1024, hop_length=512 )\n",
    "torch.save(model, 'Hello.pt')\n",
    "\n",
    "waveform, samplerate = soundfile.read( 'bonjour.wav' )\n",
    "sample_input = waveform[:32000].astype( dtype=np.float32 )\n",
    "\n",
    "# instantiate model and export it\n",
    "model = WaveToSpectrogram( n_fft=1024, hop_length=512 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "device=get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'history/Cnn10_lr_0.005_ep_100_bs_32_valoss_1.5_acc_0.94_recall_0.89_precision_0.89'\n",
    "#model_path = 'history/MobileNetV1_lr_0.005_ep_100_bs_32_valoss_1.5773910284042358_acc_0.82_recall_0.82_precision_0.82'\n",
    "waveform, samplerate = soundfile.read( '../urbansound8k/audio/fold1/101415-3-0-2.wav' )\n",
    "sample_input = waveform[:192000].astype( dtype=np.float32).reshape((1, 192000))\n",
    "\n",
    "def export_torch_onnx(model, sample_input, filename_onnx):\n",
    "    torch.onnx.export(\n",
    "            model,\n",
    "            torch.from_numpy( sample_input).to(device),\n",
    "            filename_onnx,\n",
    "            verbose = True,\n",
    "            )\n",
    "model = torch.load(os.path.join(model_path, 'model.pt'))\n",
    "export_torch_onnx(model, sample_input, os.path.join(model_path, 'model_test.onnx'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX to Coreml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = onnx_coreml.convert(model = 'wave_to_spectrogram_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlmodel = onnx_coreml.convert(\n",
    "    model = 'wave_to_spectrogram_model.onnx',\n",
    "    minimum_ios_deployment_target='13',\n",
    ")\n",
    "mlmodel.save(os.path.join(model_path, 'model.mlmodel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx_coreml\n",
    "import coremltools\n",
    "model_path = 'history/Cnn10_lr_0.005_ep_100_bs_32_valoss_1.5_acc_0.94_recall_0.89_precision_0.89'\n",
    "\n",
    "mlmodel = onnx_coreml.convert(\n",
    "    model = os.path.join(model_path, 'model.onnx'),\n",
    "    mode = 'classifier',\n",
    "    minimum_ios_deployment_target='13',\n",
    "    class_labels='labels.txt',\n",
    ")\n",
    "mlmodel.save(os.path.join(model_path, 'model.mlmodel'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch to CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_pytorch_to_coreml(model, example_input: torch.Tensor, output_filename='model.mlpackage') -> None:\n",
    "    model.eval()\n",
    "    # Trace the model with exemple_input.\n",
    "    traced_model = torch.jit.trace(model, example_input, strict=False)\n",
    "    out = traced_model(example_input)\n",
    "    # Using image_input in the inputs parameter:\n",
    "    # Convert to Core ML program using the Unified Conversion API.\n",
    "    labels = [\"air_conditioner\",\n",
    "            \"car_horn\",\n",
    "            \"children_playing\",\n",
    "            \"dog_bark\",\n",
    "            \"drilling\",\n",
    "            \"engine_idling\",\n",
    "            \"gun_shot\",\n",
    "            \"jackhammer\",\n",
    "            \"siren\",\n",
    "            \"street_music\"]\n",
    "    classifier_config_ = ct.ClassifierConfig(labels)\n",
    "\n",
    "    traced_model.eval()\n",
    "    model = ct.convert(\n",
    "        traced_model,\n",
    "        convert_to=\"mlprogram\",\n",
    "        classifier_config=classifier_config_,\n",
    "        inputs=[ct.TensorType(shape=example_input.shape)]\n",
    "    )\n",
    "    model.save(output_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobiletnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'history/Cnn10_lr_0.005_ep_100_bs_32_valoss_1.5_acc_0.94_recall_0.89_precision_0.89'\n",
    "#model_path = 'history/MobileNetV1_lr_0.005_ep_100_bs_32_valoss_1.5773910284042358_acc_0.82_recall_0.82_precision_0.82'\n",
    "waveform, samplerate = soundfile.read( '../urbansound8k/audio/fold1/101415-3-0-2.wav' )\n",
    "sample_input = waveform[:192000].astype( dtype=np.float32).reshape((1, 192000))\n",
    "\n",
    "model = torch.load(os.path.join(model_path, 'model.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Frontend ==> MIL Ops: 100%|█████████▉| 384/385 [00:00<00:00, 2069.73 ops/s]\n",
      "Running MIL Common passes:   0%|          | 0/33 [00:00<?, ? passes/s]/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:101: UserWarning: Input, 'x.1', of the source model, has been renamed to 'x_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 109.90 passes/s]\n",
      "Running MIL FP16ComputePrecision pass: 100%|██████████| 1/1 [00:00<00:00,  2.34 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00,  9.42 passes/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained version of MobileNetV2\n",
    "torch_model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "# Set the model in evaluation mode.\n",
    "torch_model.eval()\n",
    "\n",
    "# Trace the model with random data.\n",
    "example_input = torch.rand(1, 3, 224, 224) \n",
    "traced_model = torch.jit.trace(torch_model, example_input)\n",
    "out = traced_model(example_input)\n",
    "\n",
    "# Using image_input in the inputs parameter:\n",
    "# Convert to Core ML program using the Unified Conversion API.\n",
    "\n",
    "traced_model.eval()\n",
    "model = ct.convert(\n",
    "    traced_model,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[ct.TensorType(shape=example_input.shape)]\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple layer module we'll reuse in our network.\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(Layer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(*dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        return x\n",
    "\n",
    "# A simple network consisting of several base layers.\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = Layer((3, 6, 3))\n",
    "        self.layer2 = Layer((6, 16, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "model = SimpleNet()  # Instantiate the network.\n",
    "example = torch.rand(1, 3, 224, 224)  # Example input, needed by jit tracer.\n",
    "traced = torch.jit.trace(model, example)  # Generate TorchScript by tracing.\n",
    "scripted = torch.jit.script(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Pytorch with coreml \n",
    "* python3.8\n",
    "* pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
    "* pip3 install coremltools==5.0b5 protobuf==3.20.1\n",
    "* pip3 install \"numpy<1.24\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mobilenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Converting Frontend ==> MIL Ops: 100%|█████████▉| 384/385 [00:00<00:00, 2018.19 ops/s]\n",
      "Running MIL Common passes:   0%|          | 0/33 [00:00<?, ? passes/s]/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '647', of the source model, has been renamed to 'var_647' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 108.20 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 147.11 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 495/495 [00:00<00:00, 1421.09 ops/s]\n",
      "[W backend_detail.cpp:393] Warning: Backend [coreml] is not available. Execution of this Module is still possible by saving and loading on a device where the backend is available. (function codegen_backend_module)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input {\n",
      "  name: \"input_0\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      shape: 3\n",
      "      shape: 224\n",
      "      shape: 224\n",
      "      dataType: FLOAT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"var_647\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      dataType: FLOAT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata {\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.source\"\n",
      "    value: \"torch==2.0.0.dev20230104\"\n",
      "  }\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.version\"\n",
      "    value: \"5.0b5\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.backends._coreml.preprocess import (\n",
    "    CompileSpec,\n",
    "    TensorSpec,\n",
    "    CoreMLComputeUnit,\n",
    ")\n",
    "\n",
    "def mobilenetv2_spec():\n",
    "    return {\n",
    "        \"forward\": CompileSpec(\n",
    "            inputs=(\n",
    "                TensorSpec(\n",
    "                    shape=[1, 3, 224, 224],\n",
    "                ),\n",
    "            ),\n",
    "            outputs=(\n",
    "                TensorSpec(\n",
    "                    shape=[1, 1000],\n",
    "                ),\n",
    "            ),\n",
    "            backend=CoreMLComputeUnit.ALL,\n",
    "            allow_low_precision=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "model.eval()\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "model = torch.jit.trace(model, example)\n",
    "compile_spec = mobilenetv2_spec()\n",
    "mlmodel = torch._C._jit_to_backend(\"coreml\", model, compile_spec)\n",
    "mlmodel._save_for_lite_interpreter(\"./mobilenetv2_coreml.ptl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cnn10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cnn10(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_block1): ConvBlock(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block2): ConvBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block3): ConvBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block4): ConvBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc_audioset): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"history/Cnn10_lr_0.005_ep_100_bs_64_valoss_1.49_acc_0.95_recall_0.96_precision_0.96/model.pt\", map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models import MobileNetV1, Cnn10, MobileNetV2\n",
    "\n",
    "model_args = {\n",
    "      'sample_rate': 22050,\n",
    "      'window_size': 1024,\n",
    "      'hop_size': 320,\n",
    "      'mel_bins': 64,\n",
    "      'fmin': 50,\n",
    "      'fmax': 20000,\n",
    "      'classes_num': 10\n",
    "    }\n",
    "\n",
    "model_cnn10 = Cnn10(**model_args)\n",
    "model_cnn10.load_state_dict(torch.load('history/Cnn10_lr_0.005_ep_100_bs_64_valoss_1.49_acc_0.95_recall_0.96_precision_0.96/model_state.pt', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avec coremltools 6.1 on arrive à 203/204\n",
    "enleve le disque stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 200/201 [00:00<00:00, 6339.54 ops/s]\n",
      "Running MIL Common passes:   0%|          | 0/39 [00:00<?, ? passes/s]/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:107: UserWarning: Input, 'input.1', of the source model, has been renamed to 'input_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/Users/hugo/opt/anaconda3/envs/coremltools3.8/lib/python3.8/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:135: UserWarning: Output, '283', of the source model, has been renamed to 'var_283' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████| 39/39 [00:00<00:00, 358.75 passes/s]\n",
      "Running MIL FP16ComputePrecision pass: 100%|██████████| 1/1 [00:00<00:00, 12.07 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 76.81 passes/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"history/Cnn10_lr_0.005_ep_100_bs_64_valoss_1.49_acc_0.95_recall_0.96_precision_0.96/model.pt\", map_location=torch.device('cpu'))\n",
    "dat1, sampling_rate1 = librosa.load('../urbansound8k/audio/fold5/100032-3-0-0.wav')\n",
    "shape_dat1 = dat1.reshape(1, dat1.shape[0])\n",
    "input_sample = torch.Tensor(shape_dat1)\n",
    "\n",
    "convert_pytorch_to_coreml(model_cnn10, input_sample, output_filename='cnn10_96acc.mlpackage')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coremltools3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09f2296e1bd13fafdfc989bdd9574012f942b33c9bf6ca930f4e404301e975ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
